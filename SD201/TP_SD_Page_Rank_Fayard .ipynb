{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <p><center><span style=\"color:red\"> TP 1 : Page Rank Algorithm </span></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">##       <u>     1) Practising with Python </u> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of L and M "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction of a list and of it squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [1 4 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "L=np.array([1,2,3])\n",
    "M=L**2\n",
    "\n",
    "print(L,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of even integers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns the even integers of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def even(list):\n",
    "    even=[]\n",
    "    for i in list:\n",
    "        if i%2==0:\n",
    "            even.append(i)\n",
    "    return even\n",
    "\n",
    "even(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of integers in a file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that reads the integers in a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 3, 4, 4, 1, 1, 5, 5, 6, 6, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_of_file(file):\n",
    "    f=open(file,\"r\")\n",
    "    list=[]\n",
    "    for line in f:\n",
    "        u,v= [ int(x) for x in line.split() ]\n",
    "        list.append(u)\n",
    "        list.append(v)\n",
    "    return list \n",
    "\n",
    "list_of_file('./graphe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 2) Implementing PageRank in Python </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) a) Beta=1 ; eps = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define two functions that will be useful to create the function PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(r1,r2): #Return the norm1 of the vector r1-r2 (to see the difference between r1 and r2)\n",
    "    res=0\n",
    "    n=len(r1)\n",
    "    for i in range(n):\n",
    "        res+=abs(r1[i]-r2[i])\n",
    "    return res\n",
    "\n",
    "def product(d,v): #Return the product of a sparse matrice represented by a dictionnary d and a vector v \n",
    "    n=len(v)\n",
    "    res=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        l=d[i+1]\n",
    "        for x in l:\n",
    "            res[x-1]+=(1./len(l))*v[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of the function PageRank.<br />\n",
    "It enables to apply the PageRank algorithm to a graph writtend in a file. <br />\n",
    "We can adjust the Beta and the epsilon parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRank(file,beta,eps):\n",
    "    f = open(file,'r')\n",
    "    d={}\n",
    "    maxi=0\n",
    "    for line in f:                                   # creation of the sparse matrix, extracted from the file \n",
    "        u,v= [int(x) for x in line.split()]          # and research of the maximum of the graph's indexes maxi \n",
    "        if u in d:\n",
    "            d[u].append(v)\n",
    "        else:\n",
    "            d[u]=[v]\n",
    "        if u>maxi:\n",
    "            maxi=u\n",
    "        if v>maxi:\n",
    "            maxi=v\n",
    "    r=[]\n",
    "    for i in range(maxi):\n",
    "        r.append(1.0)\n",
    "    r=np.array(r)                                    # creation of the initial vector result r \n",
    "    r=r/maxi\n",
    "    p=(1-beta)*r\n",
    "    while distance(r,beta*product(d,r)+p)>eps: \n",
    "        r=beta*product(d,r)+p                     #iteration until the distance between the old and the new r is superior to eps\n",
    "    y={}\n",
    "    n=len(r)\n",
    "    for i in range(1,n+1):\n",
    "        y[\"node \"+str(i)]=r[i-1]\n",
    "    return \"There you can see the importance of each node between 0 and 1 : \", y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('There you can see the importance of each node between 0 and 1 : ',\n",
       " {'node 1': 0.28576151529947913,\n",
       "  'node 2': 0.14273834228515625,\n",
       "  'node 3': 0.14288075764973956,\n",
       "  'node 4': 0.14300028483072916,\n",
       "  'node 5': 0.14273834228515625,\n",
       "  'node 6': 0.14288075764973956})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PageRank('./graphe',1,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node 1 is the most important in so far as two nodes target the node 1. \n",
    "The others are less important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 )b) Beta=0.8 ; esp=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the parameter Beta, which represents the jump possibility and we obtain : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('There you can see the importance of each node between 0 and 1 : ',\n",
       " {'node 1': 0.27144166191245994,\n",
       "  'node 2': 0.14205073558667264,\n",
       "  'node 3': 0.14688009681043457,\n",
       "  'node 4': 0.15069667329332564,\n",
       "  'node 5': 0.14205073558667264,\n",
       "  'node 6': 0.14688009681043457})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PageRank('./graphe',0.8,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Web Graph : applying PageRank to Wikipedia pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to create the graph ourself,by exploring the html code of each Wikipedia page, and by seeing if they contains links to other pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x83 in position 37: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-33489f2ea081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#Getting the list of links in current html file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlinkList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'href=[\\'\"]?([^\\'\" >]+)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#Remove all unwanted links by intersecting with files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x83 in position 37: invalid start byte"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import html\n",
    "\n",
    "\n",
    "files = [f for f in listdir(\"./toyset\") if isfile(join(\"./toyset\", f))]\n",
    "\n",
    "g=[]\n",
    "filesSet=set(files)\n",
    "graph=\"\"\n",
    "for i in range(len(files)):\n",
    "    f=open(\"./toyset/\"+files[i],encoding=\"utf8\")\n",
    "    \n",
    "    #Getting the list of links in current html file\n",
    "    linkList=re.findall(r'href=[\\'\"]?([^\\'\" >]+)',f.read())\n",
    "    \n",
    "    #Remove all unwanted links by intersecting with files\n",
    "    linkList=list(set(linkList) & filesSet)\n",
    "    for link in linkList:\n",
    "        v=files.index(link)\n",
    "        graph+=str(i)+\" \"+str(v)+\",\"\n",
    "        \n",
    "graph=graph[:-1] #Remove the last coma\n",
    "\n",
    "f = open(\"wikipedia_graph.txt\", \"w\")\n",
    "n = f.write(graph)\n",
    "f.close()\n",
    "\n",
    "pi=PageRank(\"wikipedia_graph.txt\",0.8,0.001).toarray()[0]\n",
    "print(pi)\n",
    "for i in range(len(pi)):\n",
    "    print(files[i]+\" : \"+str(pi[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our graph, named \"graph_wiki\". We adapt a little our function PageRank to presents the results differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1e6ec1c8d547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"There you can see the importance of each page between 0 and 100 : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mPageRank_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_wiki\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1e6ec1c8d547>\u001b[0m in \u001b[0;36mPageRank_graph\u001b[0;34m(graph, beta, eps)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def PageRank_graph(graph,beta,eps):\n",
    "    d={}\n",
    "    maxi=0\n",
    "    for couple in graph:                                   # creation of the sparse matrix \n",
    "        u,v= couple[0],couple[1]\n",
    "        if u in d:\n",
    "            d[u].append(v)\n",
    "        else:\n",
    "            d[u]=[v]\n",
    "        if u>maxi:\n",
    "            maxi=u\n",
    "        if v>maxi:\n",
    "            maxi=v\n",
    "    r=[]\n",
    "    for i in range(maxi):\n",
    "        r.append(1.0)\n",
    "    r=np.array(r)\n",
    "    r=r/maxi\n",
    "    p=(1-beta)*r\n",
    "    while distance(r,beta*product(d,r)+p)>eps: \n",
    "        r=beta*product(d,r)+p #iteration until the distance between the old and the new r is superior to eps\n",
    "    n=len(r)\n",
    "    y={}\n",
    "    for i in range(1,n+1):\n",
    "        y[dico[i]]=100*r[i-1]\n",
    "    return \"There you can see the importance of each page between 0 and 100 : \", y\n",
    "\n",
    "PageRank_graph(graph_wiki,1,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
